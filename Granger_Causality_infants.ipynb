{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc7a2ca9",
   "metadata": {},
   "source": [
    "# Granger Causality Analysis for Infant EEG Data\n",
    "\n",
    "This notebook performs Granger causality analysis on infant EEG data to identify causal relationships between different brain regions. Granger causality measures whether one time series can predict another time series better than the series can predict itself.\n",
    "\n",
    "## Analysis Steps:\n",
    "1. **Data Loading and Preprocessing**\n",
    "   - Load EEG data from .edf files\n",
    "   - Filter signals (0.5-30 Hz bandpass)\n",
    "   - Extract standard EEG channels\n",
    "   - Downsample for computational efficiency\n",
    "\n",
    "2. **Granger Causality Computation**\n",
    "   - Calculate GC for all channel pairs\n",
    "   - Optimize lag parameters (max_lag=2-10)\n",
    "   - Apply significance testing (p < 0.05)\n",
    "   - Generate connectivity matrices\n",
    "\n",
    "3. **Visualization and Network Analysis**\n",
    "   - Plot connectivity heatmaps\n",
    "   - Create network graphs with thresholding\n",
    "   - Analyze node degree statistics\n",
    "   - Map brain region interactions\n",
    "\n",
    "4. **Temporal Dynamics Analysis**\n",
    "   - Segment data into time windows\n",
    "   - Track connectivity evolution over time\n",
    "   - Assess connection stability/consistency\n",
    "   - Compare different window sizes\n",
    "\n",
    "5. **Statistical Analysis and Interpretation**\n",
    "   - Identify strongest connections\n",
    "   - Analyze brain region connectivity patterns\n",
    "   - Generate summary statistics\n",
    "   - Export results for further analysis\n",
    "\n",
    "6. **Results Export and Documentation**\n",
    "   - Save connectivity matrices and statistics\n",
    "   - Generate publication-ready plots\n",
    "   - Create analysis summary reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62681f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EEG Granger Causality Analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import mne\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "from scipy import signal\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "import networkx as nx\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import multiprocessing as mp\n",
    "\n",
    "# Clean output\n",
    "warnings.filterwarnings('ignore')\n",
    "mne.set_log_level('WARNING')\n",
    "\n",
    "# Better plots\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Reproducible results\n",
    "np.random.seed(42)\n",
    "\n",
    "# CPU core detection\n",
    "N_CORES = mp.cpu_count()\n",
    "print(f\"Detected {N_CORES} CPU cores - using all for parallel processing\")\n",
    "print(\"Ready to analyze brain connectivity!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728cb3b1",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6759d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_eeg_data(file_path):\n",
    "    \"\"\"Load EEG data and show basic info\"\"\"\n",
    "    raw = mne.io.read_raw_edf(file_path, preload=True, verbose=False)\n",
    "    \n",
    "    print(f\"Loaded EEG data: {len(raw.ch_names)} channels, {raw.times[-1]:.1f}s duration\")\n",
    "    print(f\"Channels: {raw.ch_names}\")\n",
    "    \n",
    "    return raw\n",
    "\n",
    "# Load the data\n",
    "data_path = './Dataset/Infants_data/sub-NORB00005/ses-1/eeg/sub-NORB00005_ses-1_task-EEG_eeg.edf'\n",
    "raw = load_eeg_data(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c33b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_output_folder(file_path):\n",
    "    \"\"\"Create output folder based on filename\"\"\"\n",
    "    # Get filename without extension\n",
    "    filename = os.path.basename(file_path).replace('.edf', '')\n",
    "    \n",
    "    # Create output directory\n",
    "    output_dir = f'./Dataset/Infants_data_output/{filename}'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"Output folder: {output_dir}\")\n",
    "    return output_dir\n",
    "\n",
    "# Setup output directory\n",
    "output_dir = setup_output_folder(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e3608a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_eeg(raw, lowpass=30, highpass=0.5):\n",
    "    \"\"\"Clean and filter EEG data\"\"\"\n",
    "    # Filter the data\n",
    "    raw_filtered = raw.copy()\n",
    "    raw_filtered.filter(highpass, lowpass, fir_design='firwin', verbose=False)\n",
    "    \n",
    "    # Get data and extract EEG channels only\n",
    "    data, times = raw_filtered[:, :]\n",
    "    \n",
    "    # Standard EEG electrodes\n",
    "    eeg_names = ['Fp1', 'Fp2', 'F3', 'F4', 'F7', 'F8', 'FZ', 'C3', 'C4', 'CZ', \n",
    "                 'P3', 'P4', 'PZ', 'O1', 'O2', 'T3', 'T4', 'T5', 'T6']\n",
    "    \n",
    "    # Find EEG channels\n",
    "    eeg_indices = []\n",
    "    eeg_channels = []\n",
    "    \n",
    "    for i, ch in enumerate(raw_filtered.ch_names):\n",
    "        if ch in eeg_names:\n",
    "            eeg_indices.append(i)\n",
    "            eeg_channels.append(ch)\n",
    "    \n",
    "    # Extract clean EEG data\n",
    "    clean_data = data[eeg_indices, :]\n",
    "    \n",
    "    print(f\"Preprocessed: {len(eeg_channels)} EEG channels\")\n",
    "    print(f\"Channels: {eeg_channels}\")\n",
    "    print(f\"Data shape: {clean_data.shape}\")\n",
    "    \n",
    "    return clean_data, eeg_channels, times\n",
    "\n",
    "# Preprocess the data\n",
    "eeg_data, channel_names, times = preprocess_eeg(raw)\n",
    "sampling_freq = raw.info['sfreq']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a4b737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample EEG data\n",
    "def plot_eeg_sample(data, channels, times, duration=10, save_path=None):\n",
    "    \"\"\"Plot first 4 EEG channels for quick visualization\"\"\"\n",
    "    # Calculate time window\n",
    "    samples = int(duration * sampling_freq)\n",
    "    time_slice = slice(0, min(samples, data.shape[1]))\n",
    "\n",
    "    n_channels = len(channels)\n",
    "    \n",
    "    fig, axes = plt.subplots(n_channels, 1, figsize=(12, 2 * n_channels), sharex=True)\n",
    "    \n",
    "    for i in range(n_channels):\n",
    "        if i < len(channels):\n",
    "            axes[i].plot(times[time_slice], data[i, time_slice])\n",
    "            axes[i].set_title(f'{channels[i]}')\n",
    "            axes[i].set_ylabel('µV')\n",
    "            axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[-1].set_xlabel('Time (s)')\n",
    "    plt.suptitle(f'EEG Sample Data ({duration} seconds)')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Plot sample data\n",
    "plot_eeg_sample(\n",
    "    eeg_data, channel_names, times, \n",
    "    duration= int(raw.times[-1]),\n",
    "    save_path=os.path.join(output_dir, 'eeg_sample_data.png')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31da44d9",
   "metadata": {},
   "source": [
    "## 2. Granger Causality Analysis Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3642b3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data, original_fs, target_fs=100, max_duration=320):\n",
    "    \"\"\"Downsample and trim data for faster computation\"\"\"\n",
    "    # Ensure input is numpy array\n",
    "    data = np.asarray(data)\n",
    "    \n",
    "    # Skip downsampling if not needed\n",
    "    if original_fs <= target_fs:\n",
    "        processed_data = data.copy()\n",
    "        new_fs = original_fs\n",
    "    else:\n",
    "        # Downsample using scipy\n",
    "        from scipy import signal\n",
    "        n_new = int(data.shape[1] * target_fs / original_fs)\n",
    "        processed_data = np.asarray(signal.resample(data, n_new, axis=1))\n",
    "        new_fs = target_fs\n",
    "        print(f\"Downsampled: {original_fs} → {new_fs} Hz\")\n",
    "    \n",
    "    # Trim to max duration\n",
    "    max_samples = int(max_duration * new_fs)\n",
    "    if processed_data.shape[1] > max_samples:\n",
    "        processed_data = processed_data[:, :max_samples]\n",
    "        print(f\"Using first {max_duration}s: {processed_data.shape}\")\n",
    "    else:\n",
    "        print(f\"Using all data: {processed_data.shape}\")\n",
    "    \n",
    "    return processed_data, new_fs\n",
    "\n",
    "# Process the data\n",
    "eeg_subset, new_fs = prepare_data(eeg_data, sampling_freq, target_fs=100, max_duration= int(raw.times[-1])) # We can change max_duration here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d548b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_granger_causality_pair(args):\n",
    "    \"\"\"Helper function for parallel GC computation\"\"\"\n",
    "    i, j, data, channel_names, max_lag = args\n",
    "    try:\n",
    "        test_data = np.column_stack([data[j, :], data[i, :]])\n",
    "        result = grangercausalitytests(test_data, max_lag, verbose=False)\n",
    "        best_result = min(\n",
    "            (result[lag][0]['ssr_ftest'] for lag in range(1, max_lag + 1)),\n",
    "            key=lambda x: x[1]\n",
    "        )\n",
    "        return i, j, best_result[0], best_result[1]\n",
    "    except Exception as e:\n",
    "        return i, j, 0.0, 1.0\n",
    "\n",
    "def calculate_granger_causality(data, channel_names, max_lag=10, alpha=0.05):\n",
    "    \"\"\"Calculate Granger causality between all channel pairs\"\"\"\n",
    "    n_channels = len(channel_names)\n",
    "    gc_matrix = np.zeros((n_channels, n_channels))\n",
    "    p_values = np.ones((n_channels, n_channels))\n",
    "    \n",
    "    \n",
    "    # Create pairs for parallel processing\n",
    "    pairs = [(i, j, data, channel_names, max_lag) \n",
    "             for i in range(n_channels) for j in range(n_channels) if i != j]\n",
    "    \n",
    "    # Process in parallel\n",
    "    with ProcessPoolExecutor(max_workers=N_CORES) as executor:\n",
    "        results = list(executor.map(calculate_granger_causality_pair, pairs))\n",
    "    \n",
    "    # Fill matrices\n",
    "    for i, j, f_stat, p_val in results:\n",
    "        gc_matrix[i, j] = f_stat\n",
    "        p_values[i, j] = p_val\n",
    "    \n",
    "    return gc_matrix, p_values\n",
    "\n",
    "def apply_significance_threshold(gc_matrix, p_values, alpha=0.05):\n",
    "    \"\"\"Apply significance threshold to GC matrix\"\"\"\n",
    "    return np.where(p_values <= alpha, gc_matrix, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c07942",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Calculating Granger Causality...\")\n",
    "\n",
    "gc_matrix, p_values = calculate_granger_causality(eeg_subset, channel_names, max_lag=2)\n",
    "significant_gc = apply_significance_threshold(gc_matrix, p_values)\n",
    "\n",
    "n_sig, n_total = np.sum(significant_gc > 0), len(channel_names) * (len(channel_names) - 1)\n",
    "print(f\"Found {n_sig}/{n_total} significant connections ({n_sig/n_total*100:.1f}%) in full time length\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b76556",
   "metadata": {},
   "source": [
    "## 3. Visualization of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88827fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_connectivity_matrix(matrix, channel_names, title, cmap='hot', save_path=None):\n",
    "    \"\"\"Plot connectivity matrix as heatmap\"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Create heatmap with seaborn (simpler and prettier)\n",
    "    sns.heatmap(matrix, \n",
    "                xticklabels=channel_names, \n",
    "                yticklabels=channel_names,\n",
    "                cmap=cmap, \n",
    "                cbar_kws={'label': 'GC F-statistic'},\n",
    "                square=True)\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel('Target (Y)')\n",
    "    plt.ylabel('Source (X)')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Plot both matrices\n",
    "print(\"Plotting connectivity matrices...\")\n",
    "\n",
    "plot_connectivity_matrix(\n",
    "    gc_matrix, channel_names, \n",
    "    'Granger Causality Matrix (All Connections)',\n",
    "    save_path=os.path.join(output_dir, 'gc_matrix_all_connections.png')\n",
    ")\n",
    "\n",
    "plot_connectivity_matrix(\n",
    "    significant_gc, channel_names,\n",
    "    'Significant Granger Causality Connections (p < 0.05)',\n",
    "    save_path=os.path.join(output_dir, 'gc_matrix_significant_connections.png')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece8d886",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_connectivity_network(gc_matrix, channel_names, threshold_percentile=80, save_path=None):\n",
    "    \"\"\"Plot connectivity as a network graph\"\"\"\n",
    "    # Check if there are any non-zero values to calculate percentile\n",
    "    non_zero_values = gc_matrix[gc_matrix > 0]\n",
    "    if len(non_zero_values) == 0:\n",
    "        print(\"No connections found in the matrix\")\n",
    "        return None\n",
    "    \n",
    "    # Apply threshold and create graph\n",
    "    threshold = np.percentile(non_zero_values, threshold_percentile)\n",
    "    thresholded_matrix = np.where(gc_matrix >= threshold, gc_matrix, 0)\n",
    "    G = nx.from_numpy_array(thresholded_matrix, create_using=nx.DiGraph)\n",
    "    \n",
    "    # Create proper node mapping dictionary\n",
    "    node_mapping = {}\n",
    "    for i, name in enumerate(channel_names):\n",
    "        node_mapping[i] = name\n",
    "    \n",
    "    # Relabel nodes with channel names\n",
    "    G = nx.relabel_nodes(G, node_mapping)\n",
    "    \n",
    "    # Check if graph has edges\n",
    "    if G.number_of_edges() == 0:\n",
    "        print(f\"No edges found with threshold percentile {threshold_percentile}\")\n",
    "        print(f\"Try lowering the threshold_percentile (current: {threshold_percentile})\")\n",
    "        return G\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    pos = nx.circular_layout(G)\n",
    "    \n",
    "    # Calculate edge widths safely\n",
    "    edge_weights = [G[u][v]['weight'] for u, v in G.edges()]\n",
    "    max_weight = max(edge_weights) if edge_weights else 1\n",
    "    edge_widths = [weight / max_weight * 3 for weight in edge_weights]\n",
    "    \n",
    "    # Draw network\n",
    "    nx.draw(G, pos, \n",
    "            node_color='lightblue', \n",
    "            node_size=1000,\n",
    "            with_labels=True,\n",
    "            font_size=10,\n",
    "            font_weight='bold',\n",
    "            edge_color='red',\n",
    "            arrows=True,\n",
    "            arrowsize=20,\n",
    "            width=edge_widths,\n",
    "            alpha=0.7)\n",
    "    \n",
    "    plt.title(f'EEG Connectivity Network (Top {100-threshold_percentile}% connections)')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    print(f\"Network: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges\")\n",
    "    return G\n",
    "\n",
    "# Plot network\n",
    "G = plot_connectivity_network(\n",
    "    significant_gc, channel_names, \n",
    "    threshold_percentile=95,\n",
    "    save_path=os.path.join(output_dir, 'connectivity_network.png')\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e044d52",
   "metadata": {},
   "source": [
    "## 4. Statistical Analysis and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bd9d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_connectivity_statistics(gc_matrix, p_values, channel_names, alpha=0.05, save_dir=None):\n",
    "    \"\"\"Analyze connectivity statistics and create visualizations\"\"\"\n",
    "    \n",
    "    # Calculate basic stats\n",
    "    significant_mask = p_values < alpha\n",
    "    n_total = len(channel_names) * (len(channel_names) - 1)\n",
    "    n_significant = np.sum(significant_mask)\n",
    "    \n",
    "    print(f\"CONNECTIVITY SUMMARY\")\n",
    "    print(f\"Significant: {n_significant}/{n_total} ({n_significant/n_total*100:.1f}%)\")\n",
    "    \n",
    "    # Node statistics\n",
    "    out_degree = np.sum(significant_mask, axis=1)\n",
    "    in_degree = np.sum(significant_mask, axis=0)\n",
    "    \n",
    "    node_stats = pd.DataFrame({\n",
    "        'Channel': channel_names,\n",
    "        'Out': out_degree,\n",
    "        'In': in_degree,\n",
    "        'Total': out_degree + in_degree\n",
    "    }).sort_values('Total', ascending=False)\n",
    "    \n",
    "    print(f\"\\nTOP CONNECTED CHANNELS\")\n",
    "    print(node_stats.head(10).to_string(index=False))\n",
    "    \n",
    "    # Top connections\n",
    "    connections_df = None\n",
    "    if n_significant > 0:\n",
    "        # Vectorized approach for finding significant connections\n",
    "        sig_i, sig_j = np.where(significant_mask)\n",
    "        connections_df = pd.DataFrame({\n",
    "            'Source': [channel_names[i] for i in sig_i],\n",
    "            'Target': [channel_names[j] for j in sig_j],\n",
    "            'F-stat': gc_matrix[sig_i, sig_j],\n",
    "            'p-value': p_values[sig_i, sig_j]\n",
    "        }).sort_values('F-stat', ascending=False)\n",
    "        \n",
    "        print(f\"\\nTOP 10 STRONGEST CONNECTIONS\")\n",
    "        print(connections_df.head(10).to_string(index=False))\n",
    "    \n",
    "    # Create plots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Scatter plot\n",
    "    ax1.scatter(node_stats['Out'], node_stats['In'], s=100, alpha=0.7, color='steelblue')\n",
    "    for _, row in node_stats.iterrows():\n",
    "        ax1.annotate(row['Channel'], (row['Out'], row['In']), \n",
    "                    xytext=(3, 3), textcoords='offset points', fontsize=9)\n",
    "    ax1.set_xlabel('Out-degree')\n",
    "    ax1.set_ylabel('In-degree')\n",
    "    ax1.set_title('Channel Connectivity Pattern')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Bar plot\n",
    "    top10 = node_stats.head(10)\n",
    "    ax2.barh(range(len(top10)), top10['Total'], color='orange', alpha=0.7)\n",
    "    ax2.set_yticks(range(len(top10)))\n",
    "    ax2.set_yticklabels(top10['Channel'])\n",
    "    ax2.set_xlabel('Total Degree')\n",
    "    ax2.set_title('Top 10 Most Connected Channels')\n",
    "    ax2.grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_dir:\n",
    "        plt.savefig(os.path.join(save_dir, 'connectivity_statistics.png'), \n",
    "                   dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return node_stats, connections_df\n",
    "\n",
    "# Run analysis\n",
    "node_stats, connections_df = analyze_connectivity_statistics(\n",
    "    gc_matrix, p_values, channel_names, save_dir=output_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cbb4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_region_connectivity(region_pair_args):\n",
    "    \"\"\"Helper for parallel region connectivity calculation\"\"\"\n",
    "    i, j, src_region, tgt_region, channel_names, gc_matrix, p_values, ch_to_region, alpha = region_pair_args\n",
    "    \n",
    "    strengths = []\n",
    "    for src_ch in channel_names:\n",
    "        for tgt_ch in channel_names:\n",
    "            if (src_ch in ch_to_region and tgt_ch in ch_to_region and\n",
    "                ch_to_region[src_ch] == src_region and \n",
    "                ch_to_region[tgt_ch] == tgt_region and \n",
    "                src_ch != tgt_ch):\n",
    "                \n",
    "                src_idx, tgt_idx = channel_names.index(src_ch), channel_names.index(tgt_ch)\n",
    "                if p_values[src_idx, tgt_idx] < alpha:\n",
    "                    strengths.append(gc_matrix[src_idx, tgt_idx])\n",
    "    \n",
    "    return i, j, np.mean(strengths) if strengths else 0\n",
    "\n",
    "def analyze_brain_regions(gc_matrix, p_values, channel_names, alpha=0.05, save_path=None):\n",
    "    \"\"\"Analyze connectivity patterns by brain regions\"\"\"\n",
    "    \n",
    "    # Define brain regions\n",
    "    regions = {\n",
    "        'Frontal': ['Fp1', 'Fp2', 'F3', 'F4', 'F7', 'F8', 'FZ'],\n",
    "        'Central': ['C3', 'C4', 'CZ'],\n",
    "        'Parietal': ['P3', 'P4', 'PZ'],\n",
    "        'Occipital': ['O1', 'O2'],\n",
    "        'Temporal': ['T3', 'T4', 'T5', 'T6']\n",
    "    }\n",
    "    \n",
    "    # Create channel to region mapping\n",
    "    ch_to_region = {ch: region for region, channels in regions.items() \n",
    "                    for ch in channels if ch in channel_names}\n",
    "    \n",
    "    region_names = list(regions.keys())\n",
    "    region_matrix = np.zeros((len(region_names), len(region_names)))\n",
    "    \n",
    "    # Prepare parallel processing\n",
    "    region_args = []\n",
    "    for i, src_region in enumerate(region_names):\n",
    "        for j, tgt_region in enumerate(region_names):\n",
    "            region_args.append((i, j, src_region, tgt_region, channel_names, \n",
    "                              gc_matrix, p_values, ch_to_region, alpha))\n",
    "    \n",
    "    print(f\"Calculating region connectivity\")\n",
    "    \n",
    "    # Process in parallel\n",
    "    with ProcessPoolExecutor(max_workers=N_CORES) as executor:\n",
    "        results = list(executor.map(calculate_region_connectivity, region_args))\n",
    "    \n",
    "    # Fill matrix\n",
    "    for i, j, strength in results:\n",
    "        region_matrix[i, j] = strength\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(region_matrix, \n",
    "                xticklabels=region_names, \n",
    "                yticklabels=region_names,\n",
    "                annot=True, fmt='.2f', cmap='hot',\n",
    "                cbar_kws={'label': 'Mean GC Strength'})\n",
    "    \n",
    "    plt.title('Inter-Region Connectivity\\n(Mean Granger Causality Strength)')\n",
    "    plt.xlabel('Target Region')\n",
    "    plt.ylabel('Source Region')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\nBRAIN REGION CONNECTIVITY SUMMARY\")\n",
    "    for i, src in enumerate(region_names):\n",
    "        for j, tgt in enumerate(region_names):\n",
    "            if region_matrix[i, j] > 0:\n",
    "                print(f\"{src} → {tgt}: {region_matrix[i, j]:.3f}\")\n",
    "    \n",
    "    return region_matrix\n",
    "\n",
    "# Run analysis\n",
    "region_matrix = analyze_brain_regions(\n",
    "    gc_matrix, p_values, channel_names, \n",
    "    save_path=os.path.join(output_dir, 'brain_region_connectivity.png')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f188d2",
   "metadata": {},
   "source": [
    "## 5. Interpretation and Saving Results\n",
    "\n",
    "This analysis provides insights into the causal relationships between different brain regions in infant EEG data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57abd3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Saving results...\")\n",
    "\n",
    "# Save numpy arrays\n",
    "for name, data in [('gc_matrix', gc_matrix), ('p_values', p_values), ('significant_gc', significant_gc)]:\n",
    "    np.save(f\"{output_dir}/{name}.npy\", data)\n",
    "\n",
    "# Save text and CSV files  \n",
    "open(f\"{output_dir}/channel_names.txt\", 'w').write('\\n'.join(channel_names))\n",
    "if connections_df is not None: connections_df.to_csv(f\"{output_dir}/significant_connections.csv\", index=False)\n",
    "if node_stats is not None: node_stats.to_csv(f\"{output_dir}/node_statistics.csv\", index=False)\n",
    "\n",
    "print(f\"Results saved to: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121b401b",
   "metadata": {},
   "source": [
    "## 6. Time Window Comparison Analysis\n",
    "\n",
    "This section analyzes how Granger causality patterns change across different time windows within the same recording. This can reveal:\n",
    "- Temporal stability of connectivity patterns\n",
    "- Dynamic changes in brain network organization\n",
    "- Developmental fluctuations in neural interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17d4380",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_data_into_windows(data, window_length_sec, overlap_ratio=0.5, sampling_freq=100):\n",
    "    \"\"\"Segment EEG data into overlapping time windows\"\"\"\n",
    "    \n",
    "    window_samples = int(window_length_sec * sampling_freq)\n",
    "    step_samples = int(window_samples * (1 - overlap_ratio))\n",
    "    \n",
    "    windows = []\n",
    "    window_info = []\n",
    "    \n",
    "    for start_idx in range(0, data.shape[1] - window_samples + 1, step_samples):\n",
    "        end_idx = start_idx + window_samples\n",
    "        \n",
    "        # Extract window and create info\n",
    "        windows.append(data[:, start_idx:end_idx])\n",
    "        window_info.append({\n",
    "            'window_idx': len(windows) - 1,\n",
    "            'start_time': start_idx / sampling_freq,\n",
    "            'end_time': end_idx / sampling_freq,\n",
    "            'duration': window_length_sec\n",
    "        })\n",
    "    \n",
    "    print(f\"Created {len(windows)} windows of {window_length_sec}s each (overlap: {overlap_ratio*100:.0f}%)\")\n",
    "    \n",
    "    return windows, window_info\n",
    "\n",
    "# Quick test setup\n",
    "window_sizes = [2, 4, 6]  \n",
    "duration = eeg_subset.shape[1] / new_fs\n",
    "print(f\"Available data: {duration:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae679fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_LENGTH_SEC = 10\n",
    "\n",
    "def calculate_gc_for_window(args):\n",
    "    \"\"\"Helper for parallel window processing\"\"\"\n",
    "    window_idx, window_data, channel_names, max_lag = args\n",
    "    try:\n",
    "        gc_matrix, p_values = calculate_granger_causality(\n",
    "            window_data, channel_names, max_lag=max_lag\n",
    "        )\n",
    "        significant_gc = apply_significance_threshold(gc_matrix, p_values)\n",
    "        return window_idx, gc_matrix, p_values, significant_gc\n",
    "    except Exception as e:\n",
    "        n_channels = len(channel_names)\n",
    "        zero_matrix = np.zeros((n_channels, n_channels))\n",
    "        one_matrix = np.ones((n_channels, n_channels))\n",
    "        return window_idx, zero_matrix, one_matrix, zero_matrix\n",
    "\n",
    "def calculate_gc_across_windows(data, channel_names, window_length_sec=WINDOW_LENGTH_SEC, \n",
    "                                overlap_ratio=0.5, sampling_freq=100, max_lag=3):\n",
    "    \"\"\"Calculate Granger causality across multiple time windows\"\"\"\n",
    "    \n",
    "    # Segment data\n",
    "    windows, window_info = segment_data_into_windows(\n",
    "        data, window_length_sec, overlap_ratio, sampling_freq\n",
    "    )\n",
    "    \n",
    "    n_channels = len(channel_names)\n",
    "    gc_results = {\n",
    "        'gc_matrices': [None] * len(windows),\n",
    "        'p_value_matrices': [None] * len(windows),\n",
    "        'significant_matrices': [None] * len(windows),\n",
    "        'window_info': window_info\n",
    "    }\n",
    "    \n",
    "    print(f\"Calculating GC for {len(windows)} windows\")\n",
    "    \n",
    "    # Prepare parallel processing\n",
    "    window_args = [(i, window_data, channel_names, max_lag) \n",
    "                   for i, window_data in enumerate(windows)]\n",
    "    \n",
    "    # Process in parallel\n",
    "    with ProcessPoolExecutor(max_workers=N_CORES) as executor:\n",
    "        results = list(executor.map(calculate_gc_for_window, window_args))\n",
    "    \n",
    "    # Store results\n",
    "    for window_idx, gc_matrix, p_values, significant_gc in results:\n",
    "        gc_results['gc_matrices'][window_idx] = gc_matrix\n",
    "        gc_results['p_value_matrices'][window_idx] = p_values\n",
    "        gc_results['significant_matrices'][window_idx] = significant_gc\n",
    "        \n",
    "        n_connections = np.sum(significant_gc > 0)\n",
    "        print(f\"Window {window_idx+1}/{len(windows)}: ✓ {n_connections} connections\")\n",
    "    \n",
    "    return gc_results\n",
    "\n",
    "# Run analysis with your configured window length\n",
    "print(f\"ANALYZING {WINDOW_LENGTH_SEC}-SECOND WINDOWS\")\n",
    "gc_windows_custom = calculate_gc_across_windows(\n",
    "    eeg_subset, channel_names, \n",
    "    window_length_sec=WINDOW_LENGTH_SEC,\n",
    "    overlap_ratio=0.3,\n",
    "    sampling_freq=new_fs,\n",
    "    max_lag=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4483dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_connectivity_evolution(gc_results, channel_names, save_path=None):\n",
    "    \"\"\"Plot how connectivity evolves across time windows\"\"\"\n",
    "    \n",
    "    gc_matrices = gc_results['gc_matrices']\n",
    "    window_info = gc_results['window_info']\n",
    "    \n",
    "    # Calculate metrics for each window\n",
    "    metrics = []\n",
    "    for i, gc_matrix in enumerate(gc_matrices):\n",
    "        total_conn = np.sum(gc_matrix > 0)\n",
    "        mean_strength = np.mean(gc_matrix[gc_matrix > 0]) if total_conn > 0 else 0\n",
    "        \n",
    "        metrics.append({\n",
    "            'start_time': window_info[i]['start_time'],\n",
    "            'total_connections': total_conn,\n",
    "            'mean_strength': mean_strength,\n",
    "            'max_strength': np.max(gc_matrix)\n",
    "        })\n",
    "    \n",
    "    metrics_df = pd.DataFrame(metrics)\n",
    "    \n",
    "    # Create plot\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 8))\n",
    "    \n",
    "    # Plot mean strength\n",
    "    ax1.plot(metrics_df['start_time'], metrics_df['mean_strength'], 'o-', color='orange', linewidth=2)\n",
    "    ax1.set_title('Connectivity Evolution Across Time Windows')\n",
    "    ax1.set_ylabel('Mean GC Strength')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot max strength\n",
    "    ax2.plot(metrics_df['start_time'], metrics_df['max_strength'], 'o-', color='red', linewidth=2)\n",
    "    ax2.set_ylabel('Max GC Strength')\n",
    "    ax2.set_xlabel('Time (seconds)')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return metrics_df\n",
    "\n",
    "# Plot connectivity evolution\n",
    "metrics_df = plot_connectivity_evolution(\n",
    "    gc_windows_custom, channel_names,\n",
    "    save_path=os.path.join(output_dir, 'connectivity_evolution.png')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56aab02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_connectivity_stability(gc_results, channel_names, save_path=None):\n",
    "    \"\"\"Analyze stability of connectivity patterns across time windows\"\"\"\n",
    "    \n",
    "    significant_matrices = gc_results['significant_matrices']\n",
    "    n_windows = len(significant_matrices)\n",
    "    n_channels = len(channel_names)\n",
    "    \n",
    "    # Calculate connection consistency (vectorized approach)\n",
    "    consistency = np.zeros((n_channels, n_channels))\n",
    "    \n",
    "    for i in range(n_channels):\n",
    "        for j in range(n_channels):\n",
    "            if i != j:\n",
    "                consistency[i, j] = sum(matrix[i, j] > 0 for matrix in significant_matrices) / n_windows\n",
    "    \n",
    "    # Plot heatmap\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(consistency, \n",
    "                xticklabels=channel_names, \n",
    "                yticklabels=channel_names,\n",
    "                cmap='viridis', \n",
    "                vmin=0, vmax=1,\n",
    "                cbar_kws={'label': 'Connection Consistency'})\n",
    "    \n",
    "    plt.title(f'Connection Consistency Across {n_windows} Windows')\n",
    "    plt.xlabel('Target Channel')\n",
    "    plt.ylabel('Source Channel')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Find stable connections (>50% consistency)\n",
    "    stable_list = []\n",
    "    for i in range(n_channels):\n",
    "        for j in range(n_channels):\n",
    "            if i != j and consistency[i, j] > 0.5:\n",
    "                stable_list.append({\n",
    "                    'source': channel_names[i],\n",
    "                    'target': channel_names[j],\n",
    "                    'consistency': consistency[i, j]\n",
    "                })\n",
    "    \n",
    "    if stable_list:\n",
    "        stable_df = pd.DataFrame(stable_list).sort_values('consistency', ascending=False)\n",
    "        print(f\"\\nMOST STABLE CONNECTIONS ({len(stable_list)} found)\")\n",
    "        print(stable_df.head(30))\n",
    "        return consistency, stable_df\n",
    "    else:\n",
    "        print(\"\\nNo highly stable connections found (>50% consistency)\")\n",
    "        return consistency, None\n",
    "\n",
    "# Usage\n",
    "consistency_matrix, stable_connections = analyze_connectivity_stability(\n",
    "    gc_windows_custom, channel_names,\n",
    "    save_path=os.path.join(output_dir, 'connectivity_stability.png')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db3ec56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_connection_frequency(gc_results, channel_names, min_frequency=1, save_path=None):\n",
    "    \"\"\"Plot frequency of connections across time windows\"\"\"\n",
    "    \n",
    "    matrices = gc_results['significant_matrices']\n",
    "    n_windows = len(matrices)\n",
    "    n_channels = len(channel_names)\n",
    "    \n",
    "    # Count connection frequencies\n",
    "    frequencies = []\n",
    "    names = []\n",
    "    \n",
    "    for i in range(n_channels):\n",
    "        for j in range(n_channels):\n",
    "            if i != j:\n",
    "                count = sum(matrix[i, j] > 0 for matrix in matrices)\n",
    "                if count >= min_frequency:\n",
    "                    frequencies.append(count)\n",
    "                    names.append(f\"{channel_names[i]}→{channel_names[j]}\")\n",
    "    \n",
    "    if not frequencies:\n",
    "        print(f\"No connections found with frequency ≥ {min_frequency}\")\n",
    "        return None\n",
    "    \n",
    "    # Sort by frequency and take top 40\n",
    "    sorted_data = sorted(zip(frequencies, names), reverse=True)\n",
    "    top_10 = sorted_data[:40]  # Limit to top 40\n",
    "    frequencies, names = zip(*top_10)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    bars = plt.bar(range(len(frequencies)), frequencies, color='steelblue', alpha=0.7)\n",
    "    \n",
    "    plt.xlabel('Connection')\n",
    "    plt.ylabel('Frequency (Windows)')\n",
    "    plt.title(f'Top 10 Connections by Frequency Across {n_windows} Windows')\n",
    "    plt.xticks(range(len(names)), names, rotation=45, ha='right')\n",
    "    plt.grid(True, axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, freq in zip(bars, frequencies):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
    "                str(freq), ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if save_path: plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print top connections\n",
    "    print(f\"\\nTOP 10 CONNECTIONS\")\n",
    "    for i, (freq, name) in enumerate(zip(frequencies, names)):\n",
    "        print(f\"{i+1:2d}. {name}: {freq}/{n_windows} ({freq/n_windows*100:.1f}%)\")\n",
    "    \n",
    "    return frequencies, names\n",
    "\n",
    "# Usage - Fixed variable name\n",
    "result = plot_connection_frequency(\n",
    "    gc_windows_custom, channel_names,\n",
    "    min_frequency=1,\n",
    "    save_path=os.path.join(output_dir, 'connection_frequency.png')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25967892",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_window_size(args):\n",
    "    \"\"\"Helper for parallel window size processing\"\"\"\n",
    "    size, data, channel_names, sampling_freq = args\n",
    "    print(f\"Analyzing {size}s windows...\")\n",
    "    return size, calculate_gc_across_windows(\n",
    "        data, channel_names,\n",
    "        window_length_sec=size,\n",
    "        overlap_ratio=0.3,\n",
    "        sampling_freq=sampling_freq,\n",
    "        max_lag=3\n",
    "    )\n",
    "\n",
    "def compare_window_sizes(data, channel_names, window_sizes, sampling_freq):\n",
    "    \"\"\"Compare connectivity patterns across different window sizes\"\"\"\n",
    "    \n",
    "    # Prepare parallel processing\n",
    "    size_args = [(size, data, channel_names, sampling_freq) for size in window_sizes]\n",
    "    \n",
    "    print(f\"Comparing {len(window_sizes)} window sizes\")\n",
    "    \n",
    "    # Process in parallel\n",
    "    with ProcessPoolExecutor(max_workers=min(len(window_sizes), N_CORES)) as executor:\n",
    "        results = list(executor.map(process_window_size, size_args))\n",
    "    \n",
    "    return dict(results)\n",
    "\n",
    "# Smart window size selection\n",
    "duration = eeg_subset.shape[1] / new_fs\n",
    "print(f\"Available data: {duration:.1f}s\")\n",
    "\n",
    "window_sizes = [WINDOW_LENGTH_SEC - 4, WINDOW_LENGTH_SEC, WINDOW_LENGTH_SEC + 4]  # seconds\n",
    "\n",
    "print(f\"Testing windows: {window_sizes}s\")\n",
    "\n",
    "# Run comparison if we have enough data\n",
    "if duration > min(window_sizes) * 3:\n",
    "    window_comparison = compare_window_sizes(eeg_subset, channel_names, window_sizes, new_fs)\n",
    "else:\n",
    "    print(\"Insufficient data for comparison\")\n",
    "    window_comparison = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f581e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_window_size_comparison(window_comparison, channel_names, save_path=None):\n",
    "    \"\"\"Plot comparison of connectivity metrics across different window sizes\"\"\"\n",
    "    if window_comparison is None:\n",
    "        print(\"No window comparison data available\")\n",
    "        return\n",
    "    \n",
    "    window_sizes = list(window_comparison.keys())\n",
    "    \n",
    "    # Collect metrics for each window size\n",
    "    stats = {}\n",
    "    for size in window_sizes:\n",
    "        matrices = window_comparison[size]['gc_matrices']\n",
    "        connections = [np.sum(matrix > 0) for matrix in matrices]\n",
    "        strengths = [np.mean(matrix[matrix > 0]) if np.sum(matrix > 0) > 0 else 0 \n",
    "                    for matrix in matrices]\n",
    "        \n",
    "        stats[size] = {\n",
    "            'avg_strength': np.mean(strengths),\n",
    "            'std_strength': np.std(strengths),\n",
    "            'num_windows': len(matrices)\n",
    "        }\n",
    "    \n",
    "    # Create 1x2 subplot (only 2 plots now)\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    colors = ['orange', 'red']\n",
    "    \n",
    "    # Plot 1: Average strength\n",
    "    sizes_labels = [f'{size}s' for size in window_sizes]\n",
    "    avg_str = [stats[size]['avg_strength'] for size in window_sizes]\n",
    "    std_str = [stats[size]['std_strength'] for size in window_sizes]\n",
    "    \n",
    "    axes[0].bar(sizes_labels, avg_str, yerr=std_str, capsize=5, \n",
    "                alpha=0.7, color=colors[0])\n",
    "    axes[0].set_ylabel('Avg GC Strength')\n",
    "    axes[0].set_title('Strength by Window Size')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Number of windows\n",
    "    n_windows = [stats[size]['num_windows'] for size in window_sizes]\n",
    "    \n",
    "    axes[1].bar(sizes_labels, n_windows, alpha=0.7, color=colors[1])\n",
    "    axes[1].set_ylabel('Number of Windows')\n",
    "    axes[1].set_title('Windows per Size')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if save_path: plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\nWINDOW SIZE COMPARISON\")\n",
    "    for size in window_sizes:\n",
    "        s = stats[size]\n",
    "        print(f\"{size}s: {s['num_windows']} windows, \"\n",
    "              f\"avg strength: {s['avg_strength']:.3f}±{s['std_strength']:.3f}\")\n",
    "\n",
    "# Usage\n",
    "if window_comparison:\n",
    "    plot_window_size_comparison(\n",
    "        window_comparison, channel_names,\n",
    "        save_path=os.path.join(output_dir, 'window_size_comparison.png')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3824b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_temporal_dynamics(gc_results, channel_names, max_pairs=5, save_path=None):\n",
    "    \"\"\"Analyze temporal evolution of strongest connections\"\"\"\n",
    "    \n",
    "    # Get data\n",
    "    gc_matrices = gc_results['gc_matrices']\n",
    "    window_info = gc_results['window_info']\n",
    "    \n",
    "    # Find most consistent connections\n",
    "    consistency_matrix, _ = analyze_connectivity_stability(gc_results, channel_names)\n",
    "    \n",
    "    # Get top connections (simplified selection)\n",
    "    top_pairs = []\n",
    "    for i in range(len(channel_names)):\n",
    "        for j in range(len(channel_names)):\n",
    "            if i != j and consistency_matrix[i, j] > 0.2:  # Lower threshold\n",
    "                top_pairs.append((i, j, consistency_matrix[i, j]))\n",
    "    \n",
    "    # Sort and limit\n",
    "    top_pairs = sorted(top_pairs, key=lambda x: x[2], reverse=True)[:max_pairs]\n",
    "    \n",
    "    if not top_pairs:\n",
    "        print(\"No stable connections found\")\n",
    "        return None\n",
    "    \n",
    "    # Plot evolution\n",
    "    n_pairs = len(top_pairs)\n",
    "    fig, axes = plt.subplots(n_pairs, 1, figsize=(12, 2.5*n_pairs))\n",
    "    if n_pairs == 1: axes = [axes]\n",
    "    \n",
    "    for idx, (i, j, consistency) in enumerate(top_pairs):\n",
    "        # Extract time series\n",
    "        gc_values = [matrix[i, j] for matrix in gc_matrices]\n",
    "        time_points = [info['start_time'] for info in window_info]\n",
    "        \n",
    "        # Plot\n",
    "        axes[idx].plot(time_points, gc_values, 'o-', linewidth=2, markersize=4)\n",
    "        axes[idx].set_title(f'{channel_names[i]} → {channel_names[j]} (C={consistency:.2f})')\n",
    "        axes[idx].set_ylabel('GC Strength')\n",
    "        axes[idx].grid(True, alpha=0.3)\n",
    "        \n",
    "        # X-label only on last plot\n",
    "        if idx == n_pairs - 1:\n",
    "            axes[idx].set_xlabel('Time (s)')\n",
    "    \n",
    "    plt.suptitle(f'Temporal Dynamics - Top {n_pairs} Connections', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path: plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"Analyzed {n_pairs} strongest connections:\")\n",
    "    for i, j, c in top_pairs:\n",
    "        print(f\"  {channel_names[i]} → {channel_names[j]}: {c:.3f}\")\n",
    "    \n",
    "    return top_pairs\n",
    "\n",
    "# Usage (simplified)\n",
    "if 'gc_windows_custom' in locals():\n",
    "    print(\"\\n=== TEMPORAL DYNAMICS ===\")\n",
    "    temporal_pairs = analyze_temporal_dynamics(\n",
    "        gc_windows_custom, channel_names,\n",
    "        save_path=os.path.join(output_dir, 'temporal_dynamics.png')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6102f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save time window analysis results\n",
    "print(\"\\n=== SAVING RESULTS ===\")\n",
    "\n",
    "def save_results():\n",
    "    \"\"\"Save all analysis results and plots\"\"\"\n",
    "    saved_files = []\n",
    "    \n",
    "    try:\n",
    "        # Save main GC results\n",
    "        for name, data in [('gc_matrix', gc_matrix), ('p_values', p_values), ('significant_gc', significant_gc)]:\n",
    "            if name in globals():\n",
    "                np.save(os.path.join(output_dir, f'{name}.npy'), data)\n",
    "                saved_files.append(f'{name}.npy')\n",
    "        \n",
    "        # Save window analysis if available\n",
    "        if 'gc_windows_custom' in locals():\n",
    "            np.save(os.path.join(output_dir, 'window_matrices.npy'), gc_windows_custom['gc_matrices'])\n",
    "            np.save(os.path.join(output_dir, 'window_pvalues.npy'), gc_windows_custom['p_value_matrices'])\n",
    "            saved_files.extend(['window_matrices.npy', 'window_pvalues.npy'])\n",
    "        \n",
    "        # Save CSV files\n",
    "        if 'connections_df' in locals() and connections_df is not None:\n",
    "            connections_df.to_csv(os.path.join(output_dir, 'connections.csv'), index=False)\n",
    "            saved_files.append('connections.csv')\n",
    "            \n",
    "        if 'node_stats' in locals():\n",
    "            node_stats.to_csv(os.path.join(output_dir, 'node_stats.csv'), index=False)\n",
    "            saved_files.append('node_stats.csv')\n",
    "        \n",
    "        # Save channel names\n",
    "        with open(os.path.join(output_dir, 'channels.txt'), 'w') as f:\n",
    "            f.write('\\n'.join(channel_names))\n",
    "        saved_files.append('channels.txt')\n",
    "        \n",
    "        print(f\"Saved {len(saved_files)} files to: {output_dir}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving: {e}\")\n",
    "\n",
    "# Save everything\n",
    "save_results()\n",
    "print(\"\\nANALYSIS COMPLETE!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdd886c",
   "metadata": {},
   "source": [
    "## 7. Time Window Analysis Summary\n",
    "\n",
    "The time window analysis provides insights into:\n",
    "\n",
    "### Temporal Stability\n",
    "- **Connection Consistency**: How often specific connections appear across different time windows\n",
    "- **Strength Variability**: How much connection strengths fluctuate over time\n",
    "- **Network Dynamics**: Changes in overall connectivity patterns\n",
    "\n",
    "### Window Size Effects\n",
    "- **Optimal Window Length**: Balance between temporal resolution and statistical power\n",
    "- **Detection Sensitivity**: Shorter windows may miss transient connections, longer windows may average out dynamics\n",
    "- **Computational Efficiency**: Trade-off between detail and processing time\n",
    "\n",
    "### Developmental Insights\n",
    "- **Maturation Patterns**: Stable vs. variable connections may reflect different developmental stages\n",
    "- **Functional Networks**: Consistent connections likely represent established neural pathways\n",
    "- **Plasticity Indicators**: Variable connections may indicate ongoing neural plasticity\n",
    "\n",
    "### Methodological Considerations\n",
    "- **Statistical Power**: Longer windows provide more data points for robust GC estimation\n",
    "- **Temporal Resolution**: Shorter windows better capture rapid neural dynamics\n",
    "- **Overlap Strategy**: Window overlap affects independence of observations but improves temporal resolution"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
